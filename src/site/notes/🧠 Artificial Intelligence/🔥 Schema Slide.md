---
{"dg-publish":true,"permalink":"/artificial-intelligence/schema-slide/","tags":["schema"]}
---

# 1 Agents

> Ãˆ qualunque cosa che puÃ² interagire

Possiede:

- **Sensori** â†’ per percepire l'ambiente
- **Attuatori** â†’ per effettuare azioni nell'ambiente

$f:P^{*}\to A$

Dove:

- $P^{*}$ â†’ Ã¨ la storia delle percezioni
- $A$ â†’ Ã¨ l'insieme delle possibili azioni di un agent 

![allegati/Image.png|711x394](/img/user/allegati/Image.png)

+ *The First Agent*
    semplice senza valutare nessun tipo di valutazione, completa l'obbiettivo senza considerare il tempo o i cicli. Non considera i cambiamenti intermedi

```cpp
function Reflex-Vacuum-Agent( [location,status]) returns an action
  if status = Dirty then return Suck
  else if location = A then return Right
  else if location = B then return Left
```

**Rational Agents** â†’ Ã¨ un agente che sceglie le proprie azioni allo scopo di massimizzare le metriche i performance, dato lo storico delle percezioni
{ #RationalAgent}


- Non Ã¨ onnisciente
- Non prevede il futuro
- Non ha sempre successo

*AMBIENTE STATICO* â†’ ossia completamente osservabile e prevedibile (azioni deterministiche con regole chiare) incoraggiando ad imparare comportamenti fissi.

- La soluzione ottima non cambia, stesso obbiettivo
- Soluzione fragile
    - *Esempio Vespa*: che come c'Ã¨ un cambiamento riparte dall'azione che scaturisce la percezione quindi spreco di cicli inutile

## 1.1 PEAS

***P***erformance metric

***E***nviroment

***A***ctuators

***S***ensors

PiÃ¹ Ã¨ ristretto l'ambiente â†’ piÃ¹ l'agente sarÃ  semplice da costruire

- Agent Architecture = $A + S$
- Agent = Architecture + Function
+ Example Taxi

Performance metric: safety, destination, profits, legality, comfort, ....
Environment: streets/freeways, traffic, pedestrians, weather, ...
Actuators: steering, accelerator, brake, horn, speaker/display, â€¦
Sensors: camera, accelerometers, gauges, engine sensors, keyboard, GPSâ€¦

## 1.2 Enviroments - Ambienti

![allegati/Image (2).png](/img/user/allegati/Image%20(2).png)

> **Single-Agent** â†’ ossia che nell'ambiente agisce un solo agente e non deve competere con altri

> Real Agent â†’ Ã¨ come il taxi non comprende nessuna delle caratteristiche, il che lo rende molto complicato da realizzare

- **Table-Driven Agent** â†’ ossia prende decisioni consultando una **tabella di lookup** in cui, per ogni possibile percezione (o sequenza di percezioni), Ã¨ giÃ  memorizzata lâ€™azione da compiere.
    - ComplessitÃ  alta per la creazione della tabella
- **Reflex Agent** â†’ insieme di regole condizionali dirette â€œcondizione â†’ azioneâ€
{ #ReflexAgent}


![Image (3).png|755x455](/img/user/allegati/Image%20(3).png)

***Effectiveness-Efficiency Trade-off***

| **TA**                                     | **SRA**                                                            |
| ------------------------------------------ | ------------------------------------------------------------------ |
| Alti costi per gestire e creare la tabella | Riduce i costi creando solo delle condizioni di attivazione        |
| Scala esponenzialmente il tempo â†’ $O(n^2)$ | Scala linearmente il tempo â†’ $O(n)$                                |
|                                            | Semplifica un approccio giÃ  base e stupido â†’ osservabile e piccolo |

## 1.3 Tipi di Agenti

### 1.3.1 Model-Based Agent
{ #5938be}


![Image (4).png|748x383](/img/user/allegati/Image%20(4).png)

Aggiunge â†’ *STATE* al [[#^ReflexAgent]

- Stima lo stato attuale dell'ambiente basandosi sullo stato interno dell'agente
    - Lo stato interno dipende dallo storico delle percezioni
- Lo stato interno serve per catturare il modello delle transizioni dell'ambiente
    - Conseguenze dell'azione dell'agente
    - Dinamiche dell'ambiente
- *Sensor Model* â†’ come i sensori dell'agente rappresentano il mondo - rumore e limitazioni
    - Approssima l'intero stato â†’ efficace in ambianti parzialmente osservabili

### 1.3.2 Goal-Based Agent

![Image (5).png|702x430](/img/user/allegati/Image%20(5).png)

â“Massimizzare la ricompensa immediata vs massimizzare la ricompensa a lungo termine

- **Search** â†’ come Ã¨ meglio navigare tra le possibili soluzioni? Decision Tree
- **Planning** â†’ sfruttare la conoscenza del mondo per eseguire le migliori azioni

> I modelli goal-based rappresentano il loro obbiettivo (*goal*) e possono modificarlo

### 1.3.3 Utility-Based Agent

![Image (6).png|727x448](/img/user/allegati/Image%20(6).png)

> Invece di utilizzare un solo stato, assegna un punteggio ad ogni stato interno che indica quanto Ã¨ buono

- Permette di analizzare varie traiettorie e quindi tenere in considerazione non solo se arrivo al traguardo ma anche come ci arrivo â†’ permette di selezionare la soluzione migliore tra quelle esistenti risparmiando sulle mosse da fare per esempio

Ãˆ fondamentale quando:

- ci sono stati finali **non tutti equivalenti** (es. vincere con 10 mosse vs con 100 mosse),
- lâ€™ambiente Ã¨ **stocastico** (es. backgammon),
- ci sono piÃ¹ obiettivi in conflitto (es. â€œarrivare a destinazione **velocemente ma in sicurezza**â€)

### 1.3.4 Learning-Based Agent

![Image (7).png|782x329](/img/user/allegati/Image%20(7).png)

Tutti i modelli di agenti possono essere learning-based o no.

L'apprendimento puÃ² modificare qualche componente.

- **Elementi prestazionali** â†’ i precedenti agenti ricevevano il contesto e sceglievano l'azione da effettuare
- **Critic** â†’ guida l'apprendimento tramite la valutazione del comportamento attuale con la metrica delle performance esterne
- **Problem** â†’ allontanarsi dal comportamento greedy provando nuove azioni (greedy = sceglie l'opzione migliore senza considerare altre soluzioni) 

## 1.4 Summary

- **Agenti** interagiscono con **l'ambiente** tramite **attuatori** e **sensori**
- La **funzione** degli agenti â†’  [Agent = Architecture + Function](craftdocs://open?blockId=4F5E2847-6565-43CC-A12A-EB81130E4DFF&spaceId=4b28628f-7dfd-54ce-27f9-28712867f81f) â†’ descrive cosa fa l'agente in tutte le circostanze
- La **misurazione delle prestazioni** valuta la sequenza dell'ambiente
- Un [[#^RationalAgent]]
- [PEAS](./ğŸ”¥ Schema Slide.md#peas) â†’ descrive gli ambienti di attivitÃ 
- [Enviroments - Ambienti](./ğŸ”¥ Schema Slide.md#enviroments--ambienti) â†’ Osservabile, Deterministico, Episodico, Statico, Discreto
- Molti tipi di Agenti
    - [[#^ReflexAgent]]
    - [[#1.3.1 Model-Based Agent]]
    - [[#1.3.2 Goal-Based Agent]]]
    - [[#1.3.2 Goal-Based Agent]]

---

# 2 Search

> Search Alorithm â†’ dato un problema, ritorna una sequenza di azioni, che formano un path verso lo stato *goal* o *fail*

**Open-Loop** - *offline* â†’ un agente cerca la sequenza ottimale di azioni e successivamente la esegue
- *Completamente osservabile*
- *Deterministico* â†’ risposte dell'ambiente conosciute

**Closed-Loop** - *online* â†’ l'agente valuta continuamente i feedback provenienti dall'ambiente
- *parzialmente osservabile*
- *Stocastico* â†’ le risposte variano
	- Devo controllare il mondo per sapere dove sono, in quale stato

![Image (8).png|752x448](/img/user/allegati/Image%20(8).png)

> **Scelte di Design â†’ Guida veloce**

- **Deterministica e completamente osservabile** â†’ problema a stato singolo
    - l'agente sa esattamente in quel stato si troverÃ  â†’ la soluzione Ã¨ una sequenza di stati o azioni
- > **Non osservabile** â†’ problema conforme senza sensore
    - L'agente non percepisce lo stato â†’ soluzione Ã¨ una sequenza di azioni
- *Non deterministico e parzialmente osservabile** â†’ problema di contingenza
    - le percezioni prendono nuove informazioni riguardo allo stato
    - la soluzione Ã¨ un piano contingente o una policy
    - spesso ricerca interlacciata
- **Spazio degli Stati Sconosciuto** â†’ problema di esplorazione (online - closed-loop)
    - l'agente deve scoprire lo spazio degli stati

![Image (9).png|741x278](/img/user/allegati/Image%20(9).png)

![Image (10).png|738x361](/img/user/allegati/Image%20(10).png)

## 2.1 Tree Search

> Inizia dagli stati giÃ  visitati e gli espande â†’ genera gli stati successivi

- > **Frontiera** â†’ insieme di nodi non espansi che separano i nodi espansi da quelli non raggiunti
- > **Nodi Raggiunti** â†’ *frontiera* + *nodi espansi*
- > Ricerca **Uniforme**

```cpp
function Tree-Search(problem, strategy) return a solution or failure
  inizializza con initial state del problema
  loop do
    if non ci sono candidati return failure
    scegli un nodo da espandere in base alla strategia
    if nodo contine goal state return soluzione
    else espandi il nodo e aggiungi il nodo alla ricerca ad albero
  end
```

![Image (11).png](/img/user/allegati/Image%20(11).png)

![Image (12).png](/img/user/allegati/Image%20(12).png)

- **State** â†’ uno stato Ã¨ una rappresentazione di una configurazione fisica
- **Nodo** â†’ Ã¨ una struttura dati che costituisce una parte della ricerca ad albero
    - include genitori, figli. profonditÃ  e costo dei path (cammini)
- **La funzione di Espansione**
    - Crea nuovi nodi
    - Riempie i vari campi
    - Usa la funzione *Successore* del problema per creare gli stati corrispondenti

> La ricerca ad albero non corrisponde all'intero spazio degli stati â†’ un insieme di satti finito puÃ² portare ad una ricerca infinita (loops, path ridondanti)

> Nodi nella ricerca ad albero = stati dello spazio

Gli stati sono tutti diversi nello spazio

I nodi non sono unici in un albero di ricerca â†’ se abbiamo un nodo che ritorna ad un altro giÃ  esplorato devo comunque selezionarlo

### 2.1.1 Scegliere una Strategia di Ricerca

> Una strategia di ricerca Ã¨ definita **in base all'ordine con cui i nodi vengono espansi**

Una **strategia** Ã¨ **valutata** tramite queste dimensioni:

- **Completezza** â†’ trova sempre una soluzione
- **ComplessitÃ  di Tempo** â†’ numero di nodi generati o espansi
- **ComplessitÃ  dello Spazio** â†’ numero massimo di nodi in memoria
- **Ottimale** â†’ trova sempre la soluzione con meno costo

ComplessitÃ  di Tempo e Spazio sono misurati in:

- $b$ â†’ fattore massimo di ramificazione dell'albero di ricerca (quanti nodi espando)
- $d$ â†’ profonditÃ  della soluzione migliore (minor costo)
- $m$ â†’ massima profonditÃ  dello spazio degli stati (puÃ² essere $\infty$)

## 2.2 Uninformed Tree Search - Non Informata

> Una ricerca non informata usa solo le informazioni disponibili quando definiamo il problema, non sappiamo quanto siamo lontani dal goal (obbiettivo)

- Breadth-first search
- Uniform-cost search
- Depth-first search
- Depth-limited search
- Iterative deepening search

### 2.2.1 Breadth-First Search
- Frontiera â†’ Ã¨ una coda FIFO
- Espande prima in larghezza poi in profonditÃ 

![Image (13).png](/img/user/allegati/Image%20(13).png)

![Image (14).png](/img/user/allegati/Image%20(14).png)

| **Completo**    | **Time**                                         | **Space**                      | **Optimal**                                       |
| --------------- | ------------------------------------------------ | ------------------------------ | ------------------------------------------------- |
| **Si**          | $\mathbf{O}(b^d)$                                | $\mathbf{O}(b^d)$              | **Si**                                            |
| Se $b$ Ã¨ finito | Dato che fa tutti i nodi ad una certa profonditÃ  | Prende tutti i nodi in memoria | Se il costo per step Ã¨ $1$ non ottimo in generale |

#### Dijkstra's Algorithm â†’ ricerca costo uniforme

> Le azioni hanno un costo differente â†’ espandere i nodi con il cammino di costo minimo

> La frontiera Ã¨ una coda pesata â†’ i path con costo minore vanno per primi

| **Completo**                                          | **Time**                                                                                                                | **Space**                | **Optimal**                                                |
| ----------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------- | ------------------------ | ---------------------------------------------------------- |
| **Si**                                                | $\mathbf{O}(b^{1+[\frac{C^*}{\eta}]})$                                                                                  | $\mathbf{O}(b^d)$        | **Si**                                                     |
| Se il costo minimo delle azioni Ã¨ positivo ossia $>0$ | Dove $C^*$ Ã¨ il costo del path ottimale â†’ puÃ² eplorare percorsi a basso costo che non servono a niente puÃ² essere $b^d$ | **Stesso della memoria** | I nodi sono espansi in ordine in base al costo del cammino |

### 2.2.2 Depth-First Search

> Espande prima i nodi in profonditÃ 

> Coda â†’ LIFO â†’ mette i nodi in cima (Ultimo ad entrare primo ad uscire Last In - First Out)

![Image (15).png](/img/user/allegati/Image%20(15).png)

![Image (16).png](/img/user/allegati/Image%20(16).png)

| **Completo**                 | **Time**                                                                       | **Space**        | **Optimal** |
| ---------------------------- | ------------------------------------------------------------------------------ | ---------------- | ----------- |
| **Si**                       | $\mathbf{O}(b^m)$                                                              | $\mathbf{O}(bm)$ | No          |
| Se $b$ Ã¨ finito e senza loop | Dato che vado prima in profonditÃ  esploro prima tutti i fligli fino al massimo | **Lineare**      |             |

> DFS puÃ² essere implementato in vari modi:

- > **Depth-Limited search** â†’ con profonditÃ  di esplorazione fissata
- > **Iterative Deeping Search** â†’ iterazioni con valore di profonditÃ  che aumenta
- > **Bidirectional Search** â†’ iniziamo la ricerca dallo stato iniziale e dallo stato obbiettivo verso lo stato iniziale

#### Come Riconoscere uno Stato Ripetuto?

- **Ricordare gli stati raggiunti**
    - Posso riconoscere e conservare i path di costo minore
    - Se ho spazio
- Non controllo se il problema non produce path ridondanti
- **Controlla solo i loop**
    - Conserva i genitori per ogni nodo
    - Segue la catena dei parenti per vedere se ricade in un loop
    - Posso seguire la catena fino al root â†’ puntatori al padre
    - **Non c'Ã¨ bisogno di memoria in piÃ¹** solo **piÃ¹ tempo per controllare la catena**

### 2.2.3 Sommario â†’ Uninformed Search

![Image (17).png](/img/user/allegati/Image%20(17).png)

## 2.3 Heauristic - Informed Search

> Espandere i nodi piÃ¹ promettenti

> Promettenti Ã¨ una valutazione euristica â†’ propone una stima del minor costo da uno stato al nodo $n$ fino al goal

> Frontiera â†’ Ã¨ una coda pesata e ordinata in base al valore euristico

Best-First Search Algorithm:

- Greedy Search
- A* Search

### 2.3.1 Greedy Search

> Seleziono ad ogni nodo quello con il costo euristico minore

- Non ha meccanismi per trovare il cammino migliore guardando i successivi
- Potrebbe portare a cammini di costo maggiore anche se il primo passo Ã¨ piÃ¹ conveniente

| **Completo**                                                                                                | **Time**                                       | **Space**                     | **Optimal** |
| ----------------------------------------------------------------------------------------------------------- | ---------------------------------------------- | ----------------------------- | ----------- |
| No                                                                                                          | $\mathbf{O}(b^m)$                              | $\mathbf{O}(b^m)$             | No          |
| PuÃ² rimanere incastrato in un loop.<br/>Completo in uno spazio finito con il controllo degli stati ripetuti | Una buona euristca puÃ² portare a miglioramenti | Tiene tutti i nodi in memoria |             |

### 2.3.2 A* Search

> Idea â†’ evitare di espandere i cammini giÃ  costosi

> Controllo se un cammino Ã¨ piÃ¹ costoso di un altro cammino che ho giÃ  esplorato se si allora continuo il cammino con il costo minore e cosÃ¬ via

> In questo modo tengo in considerazione il cammino di costo minimo generale, in modo da poter scegliere quello minore analizzando anche cammini alternativi

**Funzione di Valutazione** â†’ $f(n)=g(n)+h(n)$

- $g(n)$ = costo fino a questo momento per raggiungere $n$ 
- $h(n)$ = costo stimato al goal da $n$ â†’ euristica greedy
- $f(n)$ = costo stimato del path migliore passando attraverso $n$ per raggiungere il goal
- Questa funzione valuta ad ogni nodo il costo decidendo come proseguire

**ComplessitÃ  di Spazio** â†’ la complessitÃ  scala con il numero di nodi espanso

- $A*$ espande tutti i nodi con $f(n)<C^*$
- $A*$ espande qualche nodo con $f(n)=C^*$
- $A*$ non espande nodi con $f(n)>C^*$
- Con $C^*$ costo del path ottimale 

**ComplessitÃ  di Tempo** â†’ Ã¨ esponenziale nell'errore di $h$

A* utilizza un euristica *ammissibile*

- $h(n)\le h^*(n)$ dove $h^*(n)$ Ã¨ il vero costo da $n$
- $h(n)\ge 0$ tale che $h(G)=0$ per ogni goal $G$
- Un euristica ammissibile non sovrastima mai la distanza dal goal

***A** Ã¨ completa**

***A$^*$* Ã¨ ottima se *h* Ã¨ ammissibile**

> **Euristica Consistente** â†’ $h(n)\le c(n,a,n \prime)+h(n \prime),\ \forall n\prime$

- PiÃ¹ forte dell'ammissibilitÃ  â†’ il costo dei cammini Ã¨ monotona crescente
    - Ossia il valore di $f(n)$ non scende mai lungo il cammino
- Quando raggiungi uno stato Ã¨ giÃ  sulla la strada ottimale per arrivare ad $n$
    - Se l'euristica fosse solo ammissibile invece potrei estrarre un nodo con un cammino sub-ottimale e doverlo riconsiderare in seguito

![Image (18).png](/img/user/allegati/Image%20(18).png)

### 2.3.3 Problema
> A* visita troppi nodi â†’ richiede che vengano esplorati ed espansi molti nodi
> â†’ **A* Pesato**: sub-optimal ma abbastanza buono
> $f(n)=g(n)+W\ h(n),\ \ W>1$
> Trova una soluzione con costo tra $C^*$ e $W\ C^*$
> -> Spesso vicina a $C^*$

#### Contorni con A*

![Image (19).png|714x431](/img/user/allegati/Image%20(19).png)

![Image (20).png](/img/user/allegati/Image%20(20).png)

![Image (21).png](/img/user/allegati/Image%20(21).png)

#### Beam Search
LaÂ **beam search**Â Ã¨ un algoritmo di ricercaÂ basato suÂ **euristiche** Â che esplora un grafo espandendo il nodo piÃ¹ promettente in un insieme limitato di nodi.
La beam search  simile aÂ [[#2.2.1 Breadth-First Search]] limitata.Â che mira a ridurre i requisiti di memoria. Nella beam search Ã¨ tenuto in considerazione solo un numero predefinito di migliori soluzioni parziali.Â Di conseguenza, Ã¨ considerato unÂ algoritmo greedy.
- Lâ€™euristica non Ã¨ necessariamente ammissibile nÃ© ottimale
- **Obiettivo tipico**: generare sequenze plausibili in problemi di decoding
- Mantiene iÂ **B migliori cammini parziali**Â in un albero di ricerca.
- Ad ogni passo: espande ognuno, valuta i successori, sceglie i B migliori, scarta gli altri.
- Non garantisce la soluzione ottimale

---
# 3 Local Search
Cosa vogliamo:
- Veloce
- Economico
- Abbastanza buono
- Ricerca non sistematica
	- Non ci interessa come ci arriviamo all'obbiettivo ma vogliamo arrivarci

> Local search restituisce una soluzione non un path
> - Non salva gli stati visitati
> - Non esplora l'intero spazio degli stati

## 3.1 Hill Climbing
> Hill-climbing puÃ² rimanere bloccato
	- Ottimo locale
	- Greedy Local Search


> [!NOTE] Gradient Descent
> Se uso il gradiente per valutare e guidare la funzione di ricerca otteniamo la discesa del gradiente.

- si blocca su i massimi locali
- Si blocca su parti pari (flat)
### 3.1.1 Soluzioni
**Random Restart**
- Riesce a scapapre dall'ottimo locale
- Completo ma molto lento
**Sideway Move** -> *movimento laterale*
- Riesce ad uscire dalle parti flat se non sono il massimo
- Fisso -> Si sposta di massimo $k$ step
![image-1.png|525x302](/img/user/allegati/image-1.png)

```c title="Hill Climbing"
function Hill-Climbing(problem) returns a state that is a local maximum
    inputs: problem, a problem definition
    local variables: current, a node, neighbor, a node
    
    current â† Make-Node(Initial-State(problem))
    loop do
        neighbor â† a highest-valued successor of current
        if Value(neighbor) â‰¤ Value(current)
        then return State(current)
        current â† neighbor
end
```

## 3.2 Simulated Annealing
> Uscire dall'ottimo locale facendo un passo random
> - un passo random non rimane mai incastrato -> ma Ã¨ lento


> [!important] Simulated Annealing
> Corrisponde a -> *hill climbing $+$ random moves*
> - I movimenti random diventano meno frequenti nel tempo
> - I movimenti casuali migliorano con il tempo
> - **Temperature** -> Ã¨ un parametro che controlla il trade-off tra hill-climbing e movimenti random
> 	- La temperatura cambia con il tempo permettendo all'inizio di fare molti passi casuali e mano a mano che vado avanti i passi casuali diminuiscono
> 	- Alla fine $T=0$ -> nessun passo casuale


### 3.2.1 Temperature Scheduling
$$
e^{\frac{x}{T}},\quad T>0,\quad x>0
$$
- $x$ Grande -> Ã¨ una mossa molto sbagliata, c'Ã¨ una bassa probabilitÃ  di accettare lo spostamento
- Aumentando $T$ -> aumento la probabilitÃ  di accettare spostamenti
Se $T\to 0$ allora anneiling diventa deterministico -> non effettua mosse casuali
- Accetta solo spostamenti buoni $e^{\frac{x}{T}}\to 0$ 
**Anneiling Scheduling** -> inizia con T grande e lo diminuisce con il passare del tempo
- Se T viene diminuite abbastanza lentamente l'algoritmo converge al *massimo globale* 
	- Se invece Ã¨ troppo lento visito tutti gli stati -> lentissimo
- *Diminuzione Esponenziale* -> $T_t = k\ T_{t-1},\quad 0<k<1$
- *Power-law Decrese* -> $T_t=\frac{T_0}{t^a}$ es: $a=1$
### 3.2.2 Local Beam Search
> Invece di prendere solo la prima soluzione prendo le prime $k$
> Espando tutto e seleziono i top-$k$ stati
> - Ha senso quando la funzione non converge sicuramente


> [!important] Funzionamento
> Ãˆ la versione stocastica della [[#Beam Search]]
> Gli stati interagiscono con gli altri -> abbandono i path con meno importanza, concentrandomi su quelli piÃ¹ promettenti
> - Non Ã¨ una ricerca parallela su $k$ path
> - *Local beam -> parte da k stati, tiene i migliori k successori globalmente.*
> - **Non garantisce ottimalitÃ **

#### Beam search decoder in NLP

NLP -> Natural Language Processing
Tutto ciÃ² che concerne l'elaborazione del linguaggio naturale -> nell'esempio la traduzione da inglese ad italiano tramite encoder (trasforma la frase in vettori-embedding) e deconder con vari livelli di linar model e softmax per avere l'output
*Esempio:*
- *Input*: how are you?
- *Output*: come stai?
![image-2.png](/img/user/allegati/image-2.png)
##### 3.3 Beam Search vs Greedy Search in NLP
**Greedy Search** -> semplicemente prende la parola piÃ¹ simile per ogni posizione
- Ogni parola Ã¨ considerata indipendente
- Funziona ma puÃ² essere migliorato
**Beam Search**
- Trova il path migliore e tiene in considerazione cosa ha giÃ  selezionato
- Prende $N$ migliori candidati per ogni posizione
- Espande gli attuali $N$ migliori candidati e prende i migliori $N$
- Nell'ultima posizione prende il miglior risultato tra gli $N$ candidati finali
##### Beam Search Decoder in NLP
Bisogna eseguire il decoder $N$ volte per ogni posizione:
- Costa tanto
- Ma il risultato spesso Ã¨ il migliore
![image-3.png](/img/user/allegati/image-3.png)
## 3.3 Ambiente Non Deterministico

> Alcune azione hanno un risultato non deterministico

Belief State (Stato di Fede) -> un insieme di stati futuri per ogni stato corrente e coppia di azioni
- Non sai in anticipo dove ti porta l'azione
Conditional Plan -> la solzuione Ã¨ un albero (if-then-else chain) non una sequenza
- Puoi eseguire la soluzione e in base al risultato dell'ambiente scegliere l'azione corretta
- **AND-OR Search Trees** -> Ã¨ la soluzione per il conditional plan
	- **AND node** -> Fornito dall'ambiente insieme al set di tutti i possibili stati successivi
	- **OR node** -> Fornito dall'agente insieme all'azione da eseguire
### 3.3.1 Esempio Erratic Vacuum World
Non deterministic action
- Se lo stato corrente Ã¨ sporco, "suck" pulisce il quadrato corrispondente ma avvolte pulisce anche il quadrato adiacente
- Se lo stato corrente Ã¨ pulito, "suck" puÃ² depositare la sporcizia
"Left" e "Right" sono deterministiche

**Belief state**
- $Result(s,a)=?$
**Conditional plan**
- Â«suckÂ»; `if 5 do [Â«rightÂ», Â«suckÂ»]`
- Starting from state 1
- $Result(1, Â«suckÂ»)= \{5, 7\}$
> In questo modo se "suck" pulisce entrambi io avrÃ² che `if 5` non sarÃ  rispettato quindi faccio un altra azione
### 3.3.2 AND-OR Search Tree
- Necessario per evitare loop
- L'algoritmo Ã¨ leggermente piÃ¹ coprente ma l'albero si espande ancora
- Esiste una versione di $A^*$ per l'albero AND-OR

**Solzuione**
Un sotto-albero dove:
-  ogni foglia Ã¨ un goal
- Ha un azione per ogni nodo OR
- Include un ramo per tutti i risultati dei nodi AND
![image-4.png|465x412](/img/user/allegati/image-4.png)

## 3.4 Searching Senza Osservazioni
- Non ci sono percezioni
- Senza sensori -> *Conformant Problem*

> L'agente deve stimale lo stato
> **Soluzione** -> Ã¨ una sequenza di azioni
> - Non puÃ² essere un conditional plan se non puoi ricevere un feedback dall'ambiente (non posso fare `if`)

**Belief Space** -> lo spazio delle rappresentazioni interne di ciÃ² che credo sia vero sul mondo (non certe)

Le azioni possono portare informazioni su possibili stati futuri
- *belief state* $= \{1,2,3,4,5,6,7,8\}$, *action*$=Â«rightÂ»$, *next belief state* $= ?$
- $\{2, 4, 6, 8\}$
#### Search in Conformant Problem
> Idea -> cercare nel belief space (completamente osservabile) invece che nello spazio degli stati (non osservabile)

- Belief state space = powerset of states -> 2ğ‘› possible beliefs instead of N
- Initial belief state = all the N states
- Le azioni *Actions(B)* che posso fare sono limitate dall'unione e l'intersezione di tutte le azioni *Actions(S)* pr ogni $S$ in $B$
- Transition model (for deterministic actions): Quando esegui unâ€™azione, non vai in un singolo stato, ma in unÂ **nuovo belief state**.
- **Goal Test**
	- **Possibly achieve**Â il goal: se almeno uno degli stati nel belief Ã¨ un goal.
	- **Necessarily achieve**Â il goal: seÂ _tutti_Â gli stati del belief sono goal.
- **Action cost**: Se lâ€™azione costa in modo diverso nei vari stati, allora devi trovare un criterio
	- Questo serve per decidere quanto â€œvaleâ€ unâ€™azione in un belief state.

##### Check Visited States
###### Come controllare gli stati visitati?
- Esempio:
	- ğµ1 = {5, 7}
	- ğµ2 = {1, 3, 5, 7}
- PuoiÂ **potare**Â (cioÃ¨ scartare) i rami di ğµ2, perchÃ© qualsiasi soluzione valida per $\{1, 3, 5, 7\}$ sarÃ  anche una soluzione per $\{5, 7\}$.
ğŸ‘‰ In generale: puoi scartare i rami che appartengono aÂ **superset**Â di belief state giÃ  visitati.
###### Ragionamento inverso
- Se hai giÃ  risolto il problema per un certo belief state, allora lo hai risolto anche per qualsiasiÂ **sottoinsieme**Â di esso.
###### ComplessitÃ 
- Nonostante queste ottimizzazioni, la ricerca nei problemi conformant (cioÃ¨ senza osservazioni, dove lâ€™agente non sa esattamente dove si trova) rimane molto costosa, perchÃ© lo spazio dei belief state Ã¨ enorme.
###### Ricerca incrementale nei belief state
1. Trova una soluzione che funziona per ilÂ **primo stato**Â in B.

2. Controlla se quella soluzione funziona anche per gli altri stati in B.
    - Se sÃ¬ â†’ fermati.
    - Se no â†’ riprova.
3. **Fallire velocemente**Â (cioÃ¨ accorgersi presto se una soluzione non funziona) puÃ² migliorare la velocitÃ  di convergenza